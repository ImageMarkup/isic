from collections import defaultdict
from typing import Iterable

from django.forms.models import ModelForm
from isic_metadata.metadata import MetadataBatch, MetadataRow
import pandas as pd
from pydantic import ValidationError as PydanticValidationError
from pydantic.main import BaseModel
from s3_file_field.widgets import S3FileInput

from isic.ingest.models import Accession, Cohort, MetadataFile


class MetadataForm(ModelForm):
    class Meta:
        model = MetadataFile
        fields = ["blob"]
        widgets = {"blob": S3FileInput(attrs={"accept": "text/csv"})}


class Problem(BaseModel):
    message: str | None = None
    context: list | None = None
    type: str | None = "error"


# A dictionary of (column name, error message) -> list of row indices with that error
ColumnRowErrors = dict[tuple[str, str], list[int]]


def validate_csv_format_and_filenames(df: pd.DataFrame, cohort: Cohort) -> list[Problem]:
    problems = []

    # TODO: duplicate columns

    if "filename" not in df.columns:
        problems.append(Problem(message="Unable to find a filename column in CSV."))
        return problems

    duplicate_filenames = df[df["filename"].duplicated()].filename.values
    if duplicate_filenames.size:
        problems.append(
            Problem(message="Duplicate filenames found.", context=list(duplicate_filenames))
        )

    matching_accessions = Accession.objects.filter(
        cohort=cohort, original_blob_name__in=df["filename"]
    ).values_list("original_blob_name")

    existing_df = pd.DataFrame((x[0] for x in matching_accessions), columns=["filename"])
    unknown_images = set(df.filename.values) - set(existing_df.filename.values)
    if unknown_images:
        problems.append(
            Problem(
                message="Encountered unknown images in the CSV.",
                context=list(unknown_images),
                type="warning",
            )
        )

    return problems


def _validate_df_consistency(batch: Iterable[dict]) -> tuple[ColumnRowErrors, list[Problem]]:
    column_error_rows: ColumnRowErrors = defaultdict(list)
    batch_problems: list[Problem] = []

    # Since rows have to be evaluated twice, we need to convert the iterator to a list
    batch = list(batch)

    for i, row in enumerate(batch):
        try:
            MetadataRow.model_validate(row)
        except PydanticValidationError as e:
            for error in e.errors():
                column = error["loc"][0]
                column_error_rows[(str(column), error["msg"])].append(i)

    # validate the metadata as a "batch". this is for all checks that span rows. since this
    # currently only applies to patient/lesion checks, we can sparsely populate the MetadataRow
    # objects to save on memory.
    try:
        MetadataBatch(
            items=[
                MetadataRow(patient_id=row.get("patient_id"), lesion_id=row.get("lesion_id"))
                for row in batch
            ]
        )
    except PydanticValidationError as e:
        for error in e.errors():
            examples = error["ctx"]["examples"] if "ctx" in error else []
            batch_problems.append(Problem(message=error["msg"], context=examples))

    # defaultdict doesn't work with django templates, see https://stackoverflow.com/a/12842716
    return dict(column_error_rows), batch_problems


def validate_internal_consistency(df: pd.DataFrame) -> tuple[ColumnRowErrors, list[Problem]]:
    return _validate_df_consistency(df.to_dict(orient="records"))


def validate_archive_consistency(
    df: pd.DataFrame, cohort: Cohort
) -> tuple[ColumnRowErrors, list[Problem]]:
    """
    Validate that a CSV is consistent with the existing cohort metadata.

    This merges the existing cohort metadata with the proposed df and validates the merged
    metadata. This allows for cross column checks e.g. an existing benign accession against
    a df with diagnosis=melanoma. It also enables cross row checks, such as verifying that
    a lesion doesn't belong to more than one patient.
    """
    # this is used to speed up the random access we need below
    df = df.set_index("filename")

    def cohort_df_merged_metadata_rows():
        """
        Yield the merged metadata rows for the cohort and df.

        The merged metadata rows are generated by iterating over the cohort accessions and
        yielding the metadata for each accession. It merges if necessary and then yields the
        merged result, remembering to omit it when yielding from the remaining rows in the
        dataframe.
        """
        accessions = cohort.accessions.values_list(
            "original_blob_name",
            "metadata",
            "lesion__private_lesion_id",
            "patient__private_patient_id",
        )

        yielded_filenames: set[str] = set()

        for original_blob_name, metadata, lesion_id, patient_id in accessions.iterator():
            # lesion/patient need to be treated as top level metadata even though
            # they're stored differently in the database.
            if lesion_id:
                metadata["lesion_id"] = lesion_id
            if patient_id:
                metadata["patient_id"] = patient_id

            in_df = original_blob_name in df.index
            if in_df:
                yield metadata | df.loc[original_blob_name].to_dict()
                # df.drop is extremely slow, so we manually keep track of the rows to skip
                yielded_filenames.add(original_blob_name)
            else:
                yield metadata

        # yield all the rows in the dataframe that don't already exist in the cohort
        for df_row in df.itertuples():
            # df_row.Index is the filename
            if df_row.Index not in yielded_filenames:
                yield df_row._asdict()

    return _validate_df_consistency(cohort_df_merged_metadata_rows())
