from collections import Counter, defaultdict
from collections.abc import Iterable
import csv
from typing import Any

from django.forms.models import ModelForm
from isic_metadata.metadata import MetadataBatch, MetadataRow
from more_itertools import chunked
from pydantic import ValidationError as PydanticValidationError
from pydantic.main import BaseModel
from s3_file_field.widgets import S3FileInput

from isic.ingest.models import Accession, Cohort, MetadataFile


class MetadataForm(ModelForm):
    class Meta:
        model = MetadataFile
        fields = ["blob"]
        widgets = {"blob": S3FileInput(attrs={"accept": "text/csv"})}


class Problem(BaseModel):
    message: str | None = None
    context: list | None = None
    type: str | None = "error"


# A dictionary of (column name, error message) -> list of row indices with that error
ColumnRowErrors = dict[tuple[str, str], list[int]]


def validate_csv_format_and_filenames(
    rows: Iterable[dict[str, Any]], cohort: Cohort
) -> list[Problem]:
    problems = []
    filenames = Counter()

    if "filename" not in rows.fieldnames:
        problems.append(Problem(message="Unable to find a filename column in CSV."))
        return problems

    filenames.update(row["filename"] for row in rows)

    if filenames.most_common(1)[0][1] > 1:
        problems.append(
            Problem(
                message="Duplicate filenames found.",
                context=[filename for filename, count in filenames.most_common() if count > 1],
            )
        )

    matching_accessions = set(
        Accession.objects.filter(cohort=cohort, original_blob_name__in=filenames.keys())
        .values_list("original_blob_name", flat=True)
        .iterator()
    )

    unknown_images = set(filenames.keys()) - matching_accessions
    if unknown_images:
        problems.append(
            Problem(
                message="Encountered unknown images in the CSV.",
                context=list(unknown_images),
                type="warning",
            )
        )

    return problems


def _validate_df_consistency(
    batch: Iterable[dict[str, Any]],
) -> tuple[ColumnRowErrors, list[Problem]]:
    column_error_rows: ColumnRowErrors = defaultdict(list)
    batch_problems: list[Problem] = []

    # since batch can be exhausted, keep track of all the batch level metadata rows
    # so we can validate them after exhausting the batch.
    metadata_rows: list[MetadataRow] = []

    for i, row in enumerate(batch, start=2):
        if row.get("patient_id") or row.get("lesion_id"):
            metadata_rows.append(
                MetadataRow(patient_id=row.get("patient_id"), lesion_id=row.get("lesion_id"))
            )

        try:
            MetadataRow.model_validate(row)
        except PydanticValidationError as e:
            for error in e.errors():
                column = error["loc"][0] if error["loc"] else ""
                column_error_rows[(str(column), error["msg"])].append(i)

    # validate the metadata as a "batch". this is for all checks that span rows. since this
    # currently only applies to patient/lesion checks, we can sparsely populate the MetadataRow
    # objects to save on memory.
    try:
        MetadataBatch(items=metadata_rows)
    except PydanticValidationError as e:
        for error in e.errors():
            examples = error["ctx"]["examples"] if "ctx" in error else []
            batch_problems.append(Problem(message=error["msg"], context=examples))

    # defaultdict doesn't work with django templates, see https://stackoverflow.com/a/12842716
    return dict(column_error_rows), batch_problems


def validate_internal_consistency(
    rows: Iterable[dict[str, Any]],
) -> tuple[ColumnRowErrors, list[Problem]]:
    return _validate_df_consistency(rows)


def validate_archive_consistency(
    rows: csv.DictReader, cohort: Cohort
) -> tuple[ColumnRowErrors, list[Problem]]:
    """
    Validate that a CSV is consistent with the existing cohort metadata.

    This merges the existing cohort metadata with the proposed df and validates the merged
    metadata. This allows for cross column checks e.g. an existing benign accession against
    a df with diagnosis=melanoma. It also enables cross row checks, such as verifying that
    a lesion doesn't belong to more than one patient.
    """

    def cohort_df_merged_metadata_rows() -> Iterable[dict[str, Any]]:
        """
        Yield the merged metadata rows for the cohort and df.

        The merged metadata rows are generated by iterating over the cohort accessions and
        yielding the metadata for each accession. It merges if necessary and then yields the
        merged result, remembering to omit it when yielding from the remaining rows in the csv.
        """
        accessions = cohort.accessions.values(
            "original_blob_name",
            "lesion__private_lesion_id",
            "patient__private_patient_id",
            *Accession.metadata_keys(),
        )

        def accession_values_to_metadata_dict(accession_values: dict[str, Any]) -> dict[str, Any]:
            """
            Return the relevant metadata values from the Accession.values dict.

            This is sort of like Accession.metadata but for a single accession retrieved
            as a dict.
            """
            if "original_blob_name" in accession_values:
                del accession_values["original_blob_name"]

            if accession_values["lesion__private_lesion_id"]:
                accession_values["lesion_id"] = accession_values["lesion__private_lesion_id"]
                del accession_values["lesion__private_lesion_id"]

            if accession_values["patient__private_patient_id"]:
                accession_values["patient_id"] = accession_values["patient__private_patient_id"]
                del accession_values["patient__private_patient_id"]

            return accession_values

        yielded_filenames: set[str] = set()

        for batch in chunked(rows, 5_000):
            accessions_batch = accessions.filter(
                original_blob_name__in=[row["filename"] for row in batch]
            )
            accessions_by_filename = {
                a["original_blob_name"]: accession_values_to_metadata_dict(a)
                for a in accessions_batch
            }

            for row in batch:
                existing = accessions_by_filename[row["filename"]]

                if existing:
                    yield existing | row
                    yielded_filenames.add(row["filename"])
                else:
                    yield row

        for row in accessions.exclude(original_blob_name__in=yielded_filenames).iterator():
            yield accession_values_to_metadata_dict(row)

    return _validate_df_consistency(cohort_df_merged_metadata_rows())
